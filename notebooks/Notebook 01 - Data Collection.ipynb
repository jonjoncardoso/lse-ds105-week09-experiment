{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<font style='font-size:1.5em'>**Notebook 01 â€“ Collecting Wikipedia data**</font>\n",
                "\n",
                "<font style='font-size:1.2em'>LSE DS105L (2022/23) - Week 09</font>\n",
                "\n",
                "**AUTHOR:**  [@jonjoncardoso](http://github.com/jonjoncardoso)\n",
                "\n",
                "**DATE:** 17 March 2023\n",
                "\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports \n",
                "Section with library imports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First import libraries I want to refer by name\n",
                "import requests\n",
                "\n",
                "# Now I import the libraries I want to refer to by alias\n",
                "import pandas as pd\n",
                "\n",
                "# Lastly, I import functions and classes I want to refer to by name but I don't want to import the whole library\n",
                "from bs4 import BeautifulSoup"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Default functions\n",
                "Section with some general functions used over the notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Getting Data\n",
                "\n",
                "Let's start by scraping some data from Wikipedia. Our focus will be on the [Machine Learning wiki entry](https://en.wikipedia.org/wiki/Machine_learning).\n",
                "\n",
                "We will be using the `requests` library to make HTTP requests and the `BeautifulSoup` library to parse the HTML."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First, I need to get the HTML from the page\n",
                "response = requests.get('https://en.wikipedia.org/wiki/Machine_learning')\n",
                "\n",
                "# Now I need to parse the HTML\n",
                "soup = BeautifulSoup(response.text, 'html.parser')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Nothing new, right? We have been using these libraries for a while now."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Parsing HTML (Pure Python)\n",
                "\n",
                "Say I want to capture the **headlines** of this page. As you know, I can do so by using the `find_all` method of the `BeautifulSoup` object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "54"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "headlines_soup = soup.find_all(attrs={'class': 'mw-headline'})\n",
                "headlines_soup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For each headline, I want to get the parent tag:\n",
                "for headline in headlines_soup:\n",
                "    print(headline.parent)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_headlines_info(headlines_soup):\n",
                "    \"\"\"\n",
                "    Parses the information from the headlines on a Wikipedia page.\n",
                "\n",
                "    The list should have one dictionary for each headline on the page.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    headlines_soup : list\n",
                "        A list of BeautifulSoup objects with the class 'mw-headline'\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    list\n",
                "        A list of dictionaries with the following keys:\n",
                "        - headline id\n",
                "        - parent tag (h2, h3, h4, h5, h6, etc.)\n",
                "        - parent tag text\n",
                "    \"\"\"\n",
                "\n",
                "    list_headlines = []\n",
                "\n",
                "    # populate df with data from the page\n",
                "\n",
                "    for headline in headlines_soup:\n",
                "        list_headlines.append({'headline id': headline.get('id'),\n",
                "                                'parent tag': headline.parent.name,\n",
                "                                'parent tag text': headline.parent.text})\n",
                "\n",
                "    return list_headlines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'headline id': 'History_and_relationships_to_other_fields',\n",
                            "  'parent tag': 'h2',\n",
                            "  'parent tag text': 'History and relationships to other fields[edit]'},\n",
                            " {'headline id': 'Artificial_intelligence',\n",
                            "  'parent tag': 'h3',\n",
                            "  'parent tag text': 'Artificial intelligence[edit]'},\n",
                            " {'headline id': 'Data_mining',\n",
                            "  'parent tag': 'h3',\n",
                            "  'parent tag text': 'Data mining[edit]'}]"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "get_headlines_info(headlines_soup)[1:4]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "|    | headline id                               | parent tag   | parent tag text                                 |\n",
                        "|---:|:------------------------------------------|:-------------|:------------------------------------------------|\n",
                        "|  0 | Overview                                  | h2           | Overview[edit]                                  |\n",
                        "|  1 | History_and_relationships_to_other_fields | h2           | History and relationships to other fields[edit] |\n",
                        "|  2 | Artificial_intelligence                   | h3           | Artificial intelligence[edit]                   |\n",
                        "|  3 | Data_mining                               | h3           | Data mining[edit]                               |\n",
                        "|  4 | Optimization                              | h3           | Optimization[edit]                              |\n"
                    ]
                }
            ],
            "source": [
                "pd.DataFrame(get_headlines_info(headlines_soup))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusions\n",
                "\n",
                "This is a WIP notebook. I will be adding more content to it as we go along."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv-ds105",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
